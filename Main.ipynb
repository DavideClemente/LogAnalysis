{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Features  Label\n",
      "0  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...      0\n",
      "1  [E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
      "2  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
      "3  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
      "4  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "traces = pd.read_csv('Raw_Logs/HDFS_v1/preprocessed/Event_traces.csv')\n",
    "labels = pd.read_csv('Raw_Logs/HDFS_v1/preprocessed/anomaly_label.csv')\n",
    "\n",
    "data = traces.merge(labels, on='BlockId')\n",
    "\n",
    "data = data[['Features', 'Label_x']]\n",
    "data.rename(columns={'Label_x': 'Label'}, inplace=True)\n",
    "data['Label'] = data['Label'].apply(lambda x: 1 if x == 'Fail' else 0)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 460047\n",
      "Training sample: 8228      [E22,E5,E5,E5,E26,E26,E26,E11,E9,E11,E9,E11,E9...\n",
      "503227      [E5,E5,E5,E22,E11,E9,E11,E9,E11,E9,E26,E26,E26]\n",
      "179673    [E5,E5,E5,E22,E11,E9,E11,E9,E11,E9,E26,E26,E26...\n",
      "106452    [E22,E5,E5,E5,E26,E26,E26,E11,E9,E11,E9,E11,E9...\n",
      "231195    [E5,E5,E5,E22,E11,E9,E11,E9,E11,E9,E26,E26,E26...\n",
      "Name: Features, dtype: object\n",
      "Test set size: 115012\n",
      "Testing sample: 309014    [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...\n",
      "332401    [E5,E5,E5,E22,E11,E9,E11,E9,E26,E11,E9,E26,E26...\n",
      "303661    [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...\n",
      "350657      [E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26]\n",
      "425054    [E22,E5,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...\n",
      "Name: Features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data['Features']\n",
    "y = data['Label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(x_train)}\")\n",
    "print(f\"Training sample: {x_train.head()}\")\n",
    "print(f\"Test set size: {len(x_test)}\")\n",
    "print(f\"Testing sample: {x_test.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Encoded Sequence:  tf.Tensor(\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization, Embedding, LSTM, Dense\n",
    "\n",
    "log_templates = pd.read_csv('Raw_Logs/HDFS_v1/preprocessed/HDFS.log_templates.csv')\n",
    "\n",
    "MAX_LEN = 50\n",
    "VOCAB_SIZE = len(log_templates['EventId'].unique()) + 1\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,  # Set your desired vocabulary size\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN  # Set your desired sequence length\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(log_templates['EventId'])\n",
    "\n",
    "x_train_vectorized = vectorize_layer(x_train)\n",
    "x_test_vectorized = vectorize_layer(x_test)\n",
    "\n",
    "print(\"Sample Encoded Sequence: \", x_train_vectorized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 32)            32000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,301\n",
      "Trainable params: 85,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, embedding_vector_length, input_length=MAX_LEN))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "14373/14377 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.9705      \n",
      "Epoch 1: val_accuracy improved from -inf to 0.97137, saving model to models\\lstm_model_best.h5\n",
      "14377/14377 [==============================] - 129s 8ms/step - loss: 0.1336 - accuracy: 0.9705 - val_loss: 0.1300 - val_accuracy: 0.9714\n",
      "Epoch 2/3\n",
      "14373/14377 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9706  \n",
      "Epoch 2: val_accuracy did not improve from 0.97137\n",
      "14377/14377 [==============================] - 119s 8ms/step - loss: 0.1331 - accuracy: 0.9706 - val_loss: 0.1300 - val_accuracy: 0.9714\n",
      "Epoch 3/3\n",
      "14372/14377 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9706  "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('models/lstm_model_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(x_train_vectorized, y_train, epochs=3, batch_size=32, validation_data=(x_test_vectorized, y_test), callbacks=[checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
