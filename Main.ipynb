{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS Anomaly Detection using LSTM\n",
    "\n",
    "## Importing data "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T17:26:29.215621Z",
     "start_time": "2025-04-14T17:26:27.775686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_PATH = os.getenv('BASE_PATH')\n",
    "\n",
    "traces = pd.read_csv(os.path.join(BASE_PATH, 'Raw_logs', 'HDFS_v1', 'preprocessed', 'Event_traces.csv'))\n",
    "labels = pd.read_csv(os.path.join(BASE_PATH, 'Raw_logs', 'HDFS_v1', 'preprocessed', 'anomaly_label.csv'))\n",
    "log_templates = pd.read_csv(os.path.join(BASE_PATH, 'Raw_logs', 'HDFS_v1', 'preprocessed', 'HDFS.log_templates.csv'))\n",
    "\n",
    "data = traces.merge(labels, on='BlockId')\n",
    "\n",
    "data = data[['Features', 'Label_x']]\n",
    "data.rename(columns={'Label_x': 'Label'}, inplace=True)\n",
    "data['Label'] = data['Label'].apply(lambda x: 1 if x == 'Fail' else 0)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "data.head()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Features  Label\n",
      "0  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...      0\n",
      "1  [E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
      "2  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
      "3  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
      "4  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...      0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                            Features  Label\n",
       "0  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...      0\n",
       "1  [E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
       "2  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
       "3  [E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...      0\n",
       "4  [E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[E5,E22,E5,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[E5,E5,E22,E5,E11,E9,E11,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(event_sequences, label, window_size_local=10, step_size_local=1):\n",
    "    x1, y1 = [], []\n",
    "    for i in range(0, len(event_sequences) - window_size_local, step_size_local):\n",
    "        x1.append(event_sequences[i: i + window_size_local])\n",
    "        y1.append(label)\n",
    "    return np.array(x1), np.array(y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "VOCAB_SIZE = len(log_templates['EventId'].unique()) + 1\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,  # Set your desired vocabulary size\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN  # Set your desired sequence length\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(log_templates['EventId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(\"variables\", \"x_train.npy\")) and os.path.exists(\n",
    "        os.path.join(\"variables\", \"y_train.npy\")):\n",
    "    x_train = np.load(os.path.join(\"variables\", \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(\"variables\", \"y_train.npy\"))\n",
    "else:\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "\n",
    "    window_size = 10\n",
    "    step_size = 1\n",
    "\n",
    "    for i in tqdm(range(len(data)), desc=\"Processing events\", unit=\"log\"):\n",
    "        raw_text = data['Features'][i][1:-1].replace(\",\", \" \")\n",
    "        x_vectorized = vectorize_layer(raw_text)\n",
    "        label = data['Label'][i]\n",
    "        x_windows, y_windows = create_sliding_windows(x_vectorized, label, window_size, step_size)\n",
    "\n",
    "        x_all.append(x_windows)\n",
    "        y_all.append(y_windows)\n",
    "\n",
    "    x_train = np.concatenate(x_all, axis=0)\n",
    "    y_train = np.concatenate(y_all, axis=0)\n",
    "\n",
    "print(f\"X_Train {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"variables\", exist_ok=True)\n",
    "np.save(os.path.join(\"variables\", \"x_train.npy\"), x_train)\n",
    "np.save(os.path.join(\"variables\", \"y_train.npy\"), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(x_train, y_train, test_size=0.2,\n",
    "                                                                            random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vectorize_layer.vocabulary_size(), embedding_vector_length))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modelDropout = Sequential()\n",
    "modelDropout.add(Embedding(vectorize_layer.vocabulary_size(), embedding_vector_length))\n",
    "modelDropout.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelDropout.add(Dense(1, activation='sigmoid'))\n",
    "modelDropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "print(model.summary())\n",
    "print(modelDropout.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_final),\n",
    "    y=y_train_final.ravel()\n",
    ")\n",
    "\n",
    "# Converter para dicion√°rio:\n",
    "weights = dict(zip(np.unique(y_train_final), class_weights))\n",
    "\n",
    "print(weights)\n",
    "\n",
    "checkpoint = ModelCheckpoint('models/lstm_model_best_dropout.keras', monitor='val_accuracy', save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "model_path = os.path.join('models', 'lstm_model_best_dropout.keras')\n",
    "if os.path.exists(model_path):\n",
    "    print('Loading existing model.')\n",
    "    modelDropout = load_model(model_path)\n",
    "else:\n",
    "    print('Model not found, training a new one.')\n",
    "    # Train the LSTM model\n",
    "    modelDropout.fit(x_train_final, y_train_final,\n",
    "                     validation_data=(x_test_final, y_test_final),\n",
    "                     epochs=3, batch_size=512,\n",
    "                     class_weight=weights,\n",
    "                     callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \" \".join(event_sequence)\n",
    "x_vectorized = vectorize_layer(text_input)\n",
    "x_windows, _ = create_sliding_windows(x_vectorized, None)\n",
    "predictions = model.predict(x_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(predictions)\n",
    "plt.title(\"Failure Probability over Time\")\n",
    "plt.xlabel(\"Window Index\")\n",
    "plt.ylabel(\"Failure Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EventId                           EventTemplate\n",
      "0      E1  [*]Adding an already existing block[*]\n",
      "1      E2        [*]Verification succeeded for[*]\n",
      "2      E3                 [*]Served block[*]to[*]\n",
      "3      E4  [*]Got exception while serving[*]to[*]\n",
      "4      E5    [*]Receiving block[*]src:[*]dest:[*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing log lines: 855118line [03:21, 4236.09line/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(log_file_path, \u001B[33m'\u001B[39m\u001B[33mr\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[32m     30\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m tqdm(file, desc=\u001B[33m\"\u001B[39m\u001B[33mProcessing log lines\u001B[39m\u001B[33m\"\u001B[39m, unit=\u001B[33m\"\u001B[39m\u001B[33mline\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m         event_id = \u001B[43mmap_log_to_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m event_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     33\u001B[39m             event_sequence.append(event_id)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mmap_log_to_event\u001B[39m\u001B[34m(log_line)\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmap_log_to_event\u001B[39m(log_line):\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlog_templates\u001B[49m\u001B[43m.\u001B[49m\u001B[43miterrows\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mRegex\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_line\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mreturn\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mEventId\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv-tfmetal/lib/python3.12/site-packages/pandas/core/frame.py:1554\u001B[39m, in \u001B[36mDataFrame.iterrows\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1552\u001B[39m using_cow = using_copy_on_write()\n\u001B[32m   1553\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m.index, \u001B[38;5;28mself\u001B[39m.values):\n\u001B[32m-> \u001B[39m\u001B[32m1554\u001B[39m     s = \u001B[43mklass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m.__finalize__(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m   1555\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m using_cow \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._mgr.is_single_block:\n\u001B[32m   1556\u001B[39m         s._mgr.add_references(\u001B[38;5;28mself\u001B[39m._mgr)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv-tfmetal/lib/python3.12/site-packages/pandas/core/series.py:584\u001B[39m, in \u001B[36mSeries.__init__\u001B[39m\u001B[34m(self, data, index, dtype, name, copy, fastpath)\u001B[39m\n\u001B[32m    582\u001B[39m         data = data.copy()\n\u001B[32m    583\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m584\u001B[39m     data = \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    586\u001B[39m     manager = _get_option(\u001B[33m\"\u001B[39m\u001B[33mmode.data_manager\u001B[39m\u001B[33m\"\u001B[39m, silent=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    587\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m manager == \u001B[33m\"\u001B[39m\u001B[33mblock\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv-tfmetal/lib/python3.12/site-packages/pandas/core/construction.py:606\u001B[39m, in \u001B[36msanitize_array\u001B[39m\u001B[34m(data, index, dtype, copy, allow_2d)\u001B[39m\n\u001B[32m    604\u001B[39m subarr = data\n\u001B[32m    605\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data.dtype == \u001B[38;5;28mobject\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m606\u001B[39m     subarr = \u001B[43mmaybe_infer_to_datetimelike\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    608\u001B[39m         object_index\n\u001B[32m    609\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m using_pyarrow_string_dtype()\n\u001B[32m    610\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m is_string_dtype(subarr)\n\u001B[32m    611\u001B[39m     ):\n\u001B[32m    612\u001B[39m         \u001B[38;5;66;03m# Avoid inference when string option is set\u001B[39;00m\n\u001B[32m    613\u001B[39m         subarr = data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv-tfmetal/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1189\u001B[39m, in \u001B[36mmaybe_infer_to_datetimelike\u001B[39m\u001B[34m(value)\u001B[39m\n\u001B[32m   1184\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[32m   1186\u001B[39m \u001B[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001B[39;00m\n\u001B[32m   1187\u001B[39m \u001B[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001B[39;00m\n\u001B[32m   1188\u001B[39m \u001B[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmaybe_convert_objects\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[return-value]\u001B[39;49;00m\n\u001B[32m   1190\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1191\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001B[39;49;00m\n\u001B[32m   1192\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#  numpy would have done it for us.\u001B[39;49;00m\n\u001B[32m   1193\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconvert_numeric\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1194\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconvert_non_numeric\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1195\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype_if_all_nat\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mM8[ns]\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1196\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2543\u001B[39m, in \u001B[36mpandas._libs.lib.maybe_convert_objects\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv-tfmetal/lib/python3.12/site-packages/numpy/_core/numeric.py:366\u001B[39m, in \u001B[36mfull\u001B[39m\u001B[34m(shape, fill_value, dtype, order, device, like)\u001B[39m\n\u001B[32m    364\u001B[39m     dtype = fill_value.dtype\n\u001B[32m    365\u001B[39m a = empty(shape, dtype, order, device=device)\n\u001B[32m--> \u001B[39m\u001B[32m366\u001B[39m \u001B[43mmultiarray\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopyto\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43munsafe\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m a\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "BASE_PATH = \"~\"\n",
    "\n",
    "log_templates = pd.read_csv(os.path.join(BASE_PATH, 'Raw_logs', 'HDFS_v1', 'preprocessed', 'HDFS.log_templates.csv'))\n",
    "print(log_templates.head())\n",
    "log_templates['Regex'] = log_templates['EventTemplate'].apply(\n",
    "    lambda t: re.compile(re.escape(t).replace(r'\\[\\*\\]', '.*')))\n",
    "\n",
    "\n",
    "def map_log_to_event(log_line):\n",
    "    for _, row in log_templates.iterrows():\n",
    "        if row['Regex'].match(log_line):\n",
    "            return row['EventId']\n",
    "    return None\n",
    "\n",
    "\n",
    "log_file_path = os.path.expanduser(os.path.join(BASE_PATH, 'Raw_logs', 'HDFS_v1', 'HDFS.log'))\n",
    "\n",
    "if not os.path.exists(log_file_path):\n",
    "    raise FileNotFoundError(f\"No such file or directory: '{log_file_path}'\")\n",
    "\n",
    "event_sequence = []\n",
    "with open(log_file_path, 'r') as file:\n",
    "    for line in tqdm(file, desc=\"Processing log lines\", unit=\"line\"):\n",
    "        event_id = map_log_to_event(line.strip())\n",
    "        if event_id is not None:\n",
    "            event_sequence.append(event_id)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
